{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After each stride-2 conv, why do we double the number of filters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> A stride 2 conv with the default padding (1) and ks (3) will reduce the activation map dimension by half. Formula: (n + 2*pad - ks)//stride + 1. As the activation map dimension reduces by half we double the number of filters. This results in no overall change in computation as the network gets deeper and deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->We want initial layers to learn simple features like edge detectors etc. There are a limited number of such features. Hence the low number of filters in the beginning.\n",
    "Each of the filters in the previous layers can be combined in exponential number of ways to detect complex patterns. For example, if previous layer has 4 filters, then next layer can learn to combine them in 2^n = 16 different ways.\n",
    "Not all of those ways will be exhaustively needed, so a rough rule of thumb is to double the number of filters as you increase layers because the number of complex patterns will be exponentially more than number of simple patterns to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What data is saved by Activation Stats for each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->An activation record contains all the necessary information required to call a procedure. An activation record may contain the following units. Stores temporary and intermediate values of an expression. Stores local data of the called procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How do we get a learner's callback after they've completed training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->The call back system in the fastai library allows you to execute any code at specific times in training. This can be extremely useful if you want to:\n",
    "\n",
    "record some parameters\n",
    "use a condition to change a parameter\n",
    "make a schedule of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Draw up the benefits and drawbacks of practicing in larger batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Benefits-- larger batch sizes make larger gradient steps than smaller batch sizes for the same number of samples seen. for the same average Euclidean norm distance from the initial weights of the model, larger batch sizes have larger variance in the distance.\n",
    "Disadvantages --> Large batch sizes may cause bad generalization. Generalization means that the neural network will perform quite well on samples outside of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Why should we avoid starting training with a high learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> If learning rate is set too high, it can cause undesirable divergent behavior in your loss function and we will skip the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What are the pros of studying with a high rate of learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs and  it can be trained in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Why do we want to end the training with a low learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train and also lower the value, the slower it moves downhill. While adopting a low learning rate may be a good option to ensure that the algorithm doesn't miss any local minima, it may also imply that the convergence is proportionally increased. This article will focus on the reasoning behind the lower value of the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
